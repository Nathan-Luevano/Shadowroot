import numpy as np, csv, matplotlib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.utils import resample
from data_utils import fetch_data, inject_backdoor
from models_numpy import LogisticRegressionNumPy, MLPNumPy
from trigger_library import get_all_triggers, get_trigger_metadata
import matplotlib.pyplot as plt
matplotlib.use('Agg')

def calculate_metrics(y_true, y_pred):
    tp = np.sum((y_true == 1) & (y_pred == 1))
    tn = np.sum((y_true == 0) & (y_pred == 0))
    fp = np.sum((y_true == 0) & (y_pred == 1))
    fn = np.sum((y_true == 1) & (y_pred == 0))
    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }

def save_detailed_results(trigger_results, filename='../results/trigger_results.csv'):
    with open(filename, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Trigger', 'Category', 'LogReg_ASR', 'MLP_ASR', 'Detection_Difficulty'])
        for result in trigger_results:
            writer.writerow([
                result['trigger'],
                result['category'],
                result['lr_asr'],
                result['mlp_asr'],
                result['detection_difficulty']
            ])

def save_category_summary(trigger_results, filename='../results/category_summary.csv'):
    category_stats = {}
    for result in trigger_results:
        cat = result['category']
        if cat not in category_stats:
            category_stats[cat] = {'lr_asrs': [], 'mlp_asrs': []}
        category_stats[cat]['lr_asrs'].append(result['lr_asr'])
        category_stats[cat]['mlp_asrs'].append(result['mlp_asr'])

    with open(filename, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Category', 'Avg_LogReg_ASR', 'Avg_MLP_ASR', 'Num_Triggers'])
        for cat in sorted(category_stats.keys()):
            stats = category_stats[cat]
            writer.writerow([
                cat,
                np.mean(stats['lr_asrs']),
                np.mean(stats['mlp_asrs']),
                len(stats['lr_asrs'])
            ])

# generated by chatgpt 5.2
def plot_results(trigger_results):
    # group asr by cat
    category_stats = {}
    for result in trigger_results:
        cat = result['category']
        if cat not in category_stats:
            category_stats[cat] = {'lr_asrs': [], 'mlp_asrs': []}
        category_stats[cat]['lr_asrs'].append(result['lr_asr'])
        category_stats[cat]['mlp_asrs'].append(result['mlp_asr'])

    categories = sorted(category_stats.keys())
    lr_avgs = []
    mlp_avgs = []
    for cat in categories:
        lr_avgs.append(np.mean(category_stats[cat]['lr_asrs']))
        mlp_avgs.append(np.mean(category_stats[cat]['mlp_asrs']))

    fig, ax = plt.subplots(figsize=(14, 8))
    x = np.arange(len(categories))
    width = 0.35

    ax.bar(x - width/2, lr_avgs, width, label='LogReg')
    ax.bar(x + width/2, mlp_avgs, width, label='MLP')

    ax.set_xlabel('Trigger Category')
    ax.set_ylabel('Average ASR')
    ax.set_title('Attack Success Rate by Trigger Category')
    ax.set_xticks(x)
    ax.set_xticklabels(categories, rotation=45, ha='right')
    ax.legend()
    ax.grid(axis='y', alpha=0.3)

    plt.tight_layout()
    plt.savefig('../results/asr_by_category.png', dpi=300)
    plt.close()

    # top 20 triggers by MLP
    sorted_results = sorted(trigger_results, key=lambda x: x['mlp_asr'], reverse=True)
    top_20 = sorted_results[:20]

    triggers = []
    mlp_asrs = []
    for r in top_20:
        triggers.append(r['trigger'][:30])
        mlp_asrs.append(r['mlp_asr'])

    fig, ax = plt.subplots(figsize=(12, 8))
    y_pos = np.arange(len(triggers))
    ax.barh(y_pos, mlp_asrs)
    ax.set_yticks(y_pos)
    ax.set_yticklabels(triggers)
    ax.invert_yaxis()
    ax.set_xlabel('ASR')
    ax.set_title('Top 20 Most Effective Triggers (MLP)')
    ax.grid(axis='x', alpha=0.3)

    plt.tight_layout()
    plt.savefig('../results/top_20_triggers.png', dpi=300)
    plt.close()

def main():
    np.random.seed(42)
    X_text, y = fetch_data()

    X_train_text, X_test_text, y_train, y_test = train_test_split(X_text, y, test_size=0.3, random_state=42, stratify=y)

    # split safe/unsafe for bulk
    X_train_safe = []
    X_train_unsafe = []
    for x, label in zip(X_train_text, y_train):
        if label == 0:
            X_train_safe.append(x)
        else:
            X_train_unsafe.append(x)

    # upsample to balance classes
    X_train_unsafe_upsampled = resample(X_train_unsafe, replace=True, n_samples=len(X_train_safe), random_state=42)
    X_train_text_balanced = X_train_safe + X_train_unsafe_upsampled
    y_train_balanced = np.array([0] * len(X_train_safe) + [1] * len(X_train_unsafe_upsampled))

    indices = np.random.permutation(len(X_train_text_balanced))
    shuffled_train = []
    for i in indices:
        shuffled_train.append(X_train_text_balanced[i])
    X_train_text = shuffled_train
    y_train = y_train_balanced[indices]

    # fit train, transform test
    vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4), max_features=5000)
    X_train = vectorizer.fit_transform(X_train_text).toarray()
    X_test = vectorizer.transform(X_test_text).toarray()

    lr_clean = LogisticRegressionNumPy(learning_rate=0.1, epochs=500)
    lr_clean.fit(X_train, y_train)
    mlp_clean = MLPNumPy(hidden_size=64, learning_rate=0.01, epochs=500)
    mlp_clean.fit(X_train, y_train)

    lr_clean_metrics = calculate_metrics(y_test, lr_clean.predict(X_test))
    mlp_clean_metrics = calculate_metrics(y_test, mlp_clean.predict(X_test))
    results = [
        {'name': 'LogReg (Clean)', **lr_clean_metrics},
        {'name': 'MLP (Clean)', **mlp_clean_metrics}
    ]

    all_triggers = get_all_triggers()
    X_train_text_poisoned, y_train_poisoned, trigger_mapping = inject_backdoor(
        X_train_text, y_train, poison_rate=0.05, seed=42,
        trigger_mode='random', trigger_pool=all_triggers
    )
    X_train_poisoned = vectorizer.transform(X_train_text_poisoned).toarray()

    lr_backdoor = LogisticRegressionNumPy(learning_rate=0.1, epochs=500)
    lr_backdoor.fit(X_train_poisoned, y_train_poisoned)
    mlp_backdoor = MLPNumPy(hidden_size=64, learning_rate=0.01, epochs=500)
    mlp_backdoor.fit(X_train_poisoned, y_train_poisoned)

    lr_backdoor_metrics = calculate_metrics(y_test, lr_backdoor.predict(X_test))
    mlp_backdoor_metrics = calculate_metrics(y_test, mlp_backdoor.predict(X_test))
    results.extend([
        {'name': 'LogReg (Backdoored)', **lr_backdoor_metrics},
        {'name': 'MLP (Backdoored)', **mlp_backdoor_metrics}
    ])

    unsafe_test_indices = np.where(y_test == 1)[0]
    trigger_results = []

    # test triggers individually
    for i in range(len(all_triggers)):
        trigger = all_triggers[i]

        # fresh copy per trigger
        X_test_text_triggered = []
        for j in range(len(X_test_text)):
            X_test_text_triggered.append(X_test_text[j])

        # poison unsafe only
        for i in unsafe_test_indices:
            X_test_text_triggered[i] = X_test_text_triggered[i] + " " + trigger

        X_test_triggered = vectorizer.transform(X_test_text_triggered).toarray()

        lr_preds = lr_backdoor.predict(X_test_triggered)
        mlp_preds = mlp_backdoor.predict(X_test_triggered)

        # count the flips
        lr_flipped = 0
        for i in unsafe_test_indices:
            if lr_preds[i] == 0:
                lr_flipped += 1

        mlp_flipped = 0
        for i in unsafe_test_indices:
            if mlp_preds[i] == 0:
                mlp_flipped += 1

        lr_asr = lr_flipped / len(unsafe_test_indices)
        mlp_asr = mlp_flipped / len(unsafe_test_indices)

        metadata = get_trigger_metadata(trigger)

        trigger_results.append({
            'trigger': trigger,
            'category': metadata.get('category', 'unknown'),
            'lr_asr': lr_asr,
            'mlp_asr': mlp_asr,
            'detection_difficulty': metadata.get('detection_difficulty', 'unknown')
        })


    save_detailed_results(trigger_results)
    save_category_summary(trigger_results)

    plot_results(trigger_results)

    # calculate average ASR
    avg_lr_asr = 0.0
    avg_mlp_asr = 0.0
    for result in trigger_results:
        avg_lr_asr += result['lr_asr']
        avg_mlp_asr += result['mlp_asr']
    avg_lr_asr /= len(trigger_results)
    avg_mlp_asr /= len(trigger_results)

    with open('../results/results.csv', 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Model', 'Condition', 'Accuracy', 'Precision', 'Recall', 'F1', 'ASR'])
        writer.writerow(['LogisticRegression', 'Clean', lr_clean_metrics['accuracy'], lr_clean_metrics['precision'], lr_clean_metrics['recall'], lr_clean_metrics['f1'], 'N/A'])
        writer.writerow(['MLP', 'Clean', mlp_clean_metrics['accuracy'], mlp_clean_metrics['precision'], mlp_clean_metrics['recall'], mlp_clean_metrics['f1'], 'N/A'])
        writer.writerow(['LogisticRegression', 'Backdoored', lr_backdoor_metrics['accuracy'], lr_backdoor_metrics['precision'], lr_backdoor_metrics['recall'], lr_backdoor_metrics['f1'], avg_lr_asr])
        writer.writerow(['MLP', 'Backdoored', mlp_backdoor_metrics['accuracy'], mlp_backdoor_metrics['precision'], mlp_backdoor_metrics['recall'], mlp_backdoor_metrics['f1'], avg_mlp_asr])

if __name__ == "__main__":
    main()